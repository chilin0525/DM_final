{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Copyright (c) Recommenders contributors.</i>\n",
    "\n",
    "<i>Licensed under the MIT License.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DKN : Deep Knowledge-Aware Network for News Recommendation\n",
    "\n",
    "DKN \\[1\\] is a deep learning model which incorporates information from knowledge graph for better news recommendation. Specifically, DKN uses TransX \\[2\\] method for knowledge graph representation learning, then applies a CNN framework, named KCNN, to combine entity embedding with word embedding and generate a final embedding vector for a news article. CTR prediction is made via an attention-based neural scorer. \n",
    "\n",
    "## Properties of DKN:\n",
    "\n",
    "- DKN is a content-based deep model for CTR prediction rather than traditional ID-based collaborative filtering. \n",
    "- It makes use of knowledge entities and common sense in news content via joint learning from semantic-level and knowledge-level representations of news articles.\n",
    "- DKN uses an attention module to dynamically calculate a user's aggregated historical representation.\n",
    "\n",
    "\n",
    "## Data format\n",
    "\n",
    "DKN takes several files as input as follows:\n",
    "\n",
    "- **training / validation / test files**: each line in these files represents one instance. Impressionid is used to evaluate performance within an impression session, so it is only used when evaluating, you can set it to 0 for training data. The format is : <br> \n",
    "`[label] [userid] [CandidateNews]%[impressionid] `<br> \n",
    "e.g., `1 train_U1 N1%0` <br> \n",
    "\n",
    "- **user history file**: each line in this file represents a users' click history. You need to set `history_size` parameter in the config file, which is the max number of user's click history we use. We will automatically keep the last `history_size` number of user click history, if user's click history is more than `history_size`, and we will automatically pad with 0 if user's click history is less than `history_size`. the format is : <br> \n",
    "`[Userid] [newsid1,newsid2...]`<br>\n",
    "e.g., `train_U1 N1,N2` <br> \n",
    "\n",
    "- **document feature file**: It contains the word and entity features for news articles. News articles are represented by aligned title words and title entities. To take a quick example, a news title may be: <i>\"Trump to deliver State of the Union address next week\"</i>, then the title words value may be `CandidateNews:34,45,334,23,12,987,3456,111,456,432` and the title entitie value may be: `entity:45,0,0,0,0,0,0,0,0,0`. Only the first value of entity vector is non-zero due to the word \"Trump\". The title value and entity value is hashed from 1 to `n` (where `n` is the number of distinct words or entities). Each feature length should be fixed at k (`doc_size` parameter), if the number of words in document is more than k, you should truncate the document to k words, and if the number of words in document is less than k, you should pad 0 to the end. \n",
    "the format is like: <br> \n",
    "`[Newsid] [w1,w2,w3...wk] [e1,e2,e3...ek]`\n",
    "\n",
    "- **word embedding/entity embedding/ context embedding files**: These are `*.npy` files of pretrained embeddings. After loading, each file is a `[n+1,k]` two-dimensional matrix, n is the number of words(or entities) of their hash dictionary, k is dimension of the embedding, note that we keep embedding 0 for zero padding. \n",
    "\n",
    "In this experiment, we used GloVe\\[4\\] vectors to initialize the word embedding. We trained entity embedding using TransE\\[2\\] on knowledge graph and context embedding is the average of the entity's neighbors in the knowledge graph.<br>\n",
    "\n",
    "## MIND dataset\n",
    "\n",
    "MIND dataset\\[3\\] is a large-scale English news dataset. It was collected from anonymized behavior logs of Microsoft News website. MIND contains 1,000,000 users, 161,013 news articles and 15,777,377 impression logs. Every news article contains rich textual content including title, abstract, body, category and entities. Each impression log contains the click events, non-clicked events and historical news click behaviors of this user before this impression.\n",
    "\n",
    "A smaller version, [MIND-small](https://azure.microsoft.com/en-us/services/open-datasets/catalog/microsoft-news-dataset/), is a small version of the MIND dataset by randomly sampling 50,000 users and their behavior logs from the MIND dataset.\n",
    "\n",
    "The datasets contains these files for both training and validation data:\n",
    "\n",
    "#### behaviors.tsv\n",
    "\n",
    "The behaviors.tsv file contains the impression logs and users' news click hostories. It has 5 columns divided by the tab symbol:\n",
    "\n",
    "+ Impression ID. The ID of an impression.\n",
    "+ User ID. The anonymous ID of a user.\n",
    "+ Time. The impression time with format \"MM/DD/YYYY HH:MM:SS AM/PM\".\n",
    "+ History. The news click history (ID list of clicked news) of this user before this impression.\n",
    "+ Impressions. List of news displayed in this impression and user's click behaviors on them (1 for click and 0 for non-click).\n",
    "\n",
    "One simple example: \n",
    "\n",
    "`1    U82271    11/11/2019 3:28:58 PM    N3130 N11621 N12917 N4574 N12140 N9748    N13390-0 N7180-0 N20785-0 N6937-0 N15776-0 N25810-0 N20820-0 N6885-0 N27294-0 N18835-0 N16945-0 N7410-0 N23967-0 N22679-0 N20532-0 N26651-0 N22078-0 N4098-0 N16473-0 N13841-0 N15660-0 N25787-0 N2315-0 N1615-0 N9087-0 N23880-0 N3600-0 N24479-0 N22882-0 N26308-0 N13594-0 N2220-0 N28356-0 N17083-0 N21415-0 N18671-0 N9440-0 N17759-0 N10861-0 N21830-0 N8064-0 N5675-0 N15037-0 N26154-0 N15368-1 N481-0 N3256-0 N20663-0 N23940-0 N7654-0 N10729-0 N7090-0 N23596-0 N15901-0 N16348-0 N13645-0 N8124-0 N20094-0 N27774-0 N23011-0 N14832-0 N15971-0 N27729-0 N2167-0 N11186-0 N18390-0 N21328-0 N10992-0 N20122-0 N1958-0 N2004-0 N26156-0 N17632-0 N26146-0 N17322-0 N18403-0 N17397-0 N18215-0 N14475-0 N9781-0 N17958-0 N3370-0 N1127-0 N15525-0 N12657-0 N10537-0 N18224-0 `\n",
    "\n",
    "#### news.tsv\n",
    "\n",
    "The news.tsv file contains the detailed information of news articles involved in the behaviors.tsv file. It has 7 columns, which are divided by the tab symbol:\n",
    "\n",
    "+ News ID\n",
    "+ Category\n",
    "+ SubCategory\n",
    "+ Title\n",
    "+ Abstract\n",
    "+ URL\n",
    "+ Title Entities (entities contained in the title of this news)\n",
    "+ Abstract Entities (entites contained in the abstract of this news)\n",
    "\n",
    "One simple example: \n",
    "\n",
    "`N46466    lifestyle    lifestyleroyals    The Brands Queen Elizabeth, Prince Charles, and Prince Philip Swear By    Shop the notebooks, jackets, and more that the royals can't live without.    https://www.msn.com/en-us/lifestyle/lifestyleroyals/the-brands-queen-elizabeth,-prince-charles,-and-prince-philip-swear-by/ss-AAGH0ET?ocid=chopendata    [{\"Label\": \"Prince Philip, Duke of Edinburgh\", \"Type\": \"P\", \"WikidataId\": \"Q80976\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [48], \"SurfaceForms\": [\"Prince Philip\"]}, {\"Label\": \"Charles, Prince of Wales\", \"Type\": \"P\", \"WikidataId\": \"Q43274\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [28], \"SurfaceForms\": [\"Prince Charles\"]}, {\"Label\": \"Elizabeth II\", \"Type\": \"P\", \"WikidataId\": \"Q9682\", \"Confidence\": 0.97, \"OccurrenceOffsets\": [11], \"SurfaceForms\": [\"Queen Elizabeth\"]}]    [] `\n",
    "\n",
    "#### entity_embedding.vec & relation_embedding.vec\n",
    "\n",
    "The entity_embedding.vec and relation_embedding.vec files contain the 100-dimensional embeddings of the entities and relations learned from the subgraph (from WikiData knowledge graph) by TransE method. In both files, the first column is the ID of entity/relation, and the other columns are the embedding vector values.\n",
    "\n",
    "One simple example: \n",
    "\n",
    "`Q42306013  0.014516 -0.106958 0.024590 ... -0.080382`\n",
    "\n",
    "\n",
    "## DKN architecture\n",
    "\n",
    "The following figure shows the architecture of DKN.\n",
    "\n",
    "![](https://recodatasets.z20.web.core.windows.net/images/dkn_architecture.png)\n",
    "\n",
    "DKN takes one piece of candidate news and one piece of a user’s clicked news as input. For each piece of news, a specially designed KCNN is used to process its title and generate an embedding vector. KCNN is an extension of traditional CNN that allows flexibility in incorporating symbolic knowledge from a knowledge graph into sentence representation learning. \n",
    "\n",
    "With the KCNN, we obtain a set of embedding vectors for a user’s clicked history. To get final embedding of the user with\n",
    "respect to the current candidate news, we use an attention-based method to automatically match the candidate news to each piece\n",
    "of his clicked news, and aggregate the user’s historical interests with different weights. The candidate news embedding and the user embedding are concatenated and fed into a deep neural network (DNN) to calculate the predicted probability that the user will click the candidate news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global settings and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-16 23:57:57.040376: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-16 23:57:57.108327: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-16 23:57:57.538949: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64:/usr/local/cuda-11.7/lib64\n",
      "2024-06-16 23:57:57.538986: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64:/usr/local/cuda-11.7/lib64\n",
      "2024-06-16 23:57:57.538989: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.9.13 (main, Apr 27 2024, 15:35:35) \n",
      "[GCC 9.4.0]\n",
      "Tensorflow version: 2.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from tempfile import TemporaryDirectory\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel('ERROR') # only show error messages\n",
    "\n",
    "from recommenders.datasets.download_utils import maybe_download\n",
    "from recommenders.datasets.mind import (download_mind, \n",
    "                                     extract_mind, \n",
    "                                     read_clickhistory, \n",
    "                                     get_train_input, \n",
    "                                     get_valid_input, \n",
    "                                     get_user_history,\n",
    "                                     get_words_and_entities,\n",
    "                                     generate_embeddings) \n",
    "from recommenders.models.deeprec.deeprec_utils import prepare_hparams\n",
    "from recommenders.models.deeprec.models.dkn import DKN\n",
    "from recommenders.models.deeprec.io.dkn_iterator import DKNTextIterator\n",
    "from recommenders.utils.notebook_utils import store_metadata\n",
    "\n",
    "print(f\"System version: {sys.version}\")\n",
    "print(f\"Tensorflow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temp dir\n",
    "tmpdir = TemporaryDirectory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DKN parameters\n",
    "epochs = 10\n",
    "history_size = 50\n",
    "batch_size = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Mind parameters\n",
    "# MIND_SIZE = \"small\"\n",
    "\n",
    "\n",
    "# Paths\n",
    "# data_path = os.path.join(tmpdir.name, \"mind-dkn\")\n",
    "# train_file = os.path.join(data_path, \"train_mind.txt\")\n",
    "# valid_file = os.path.join(data_path, \"valid_mind.txt\")\n",
    "# user_history_file = os.path.join(data_path, \"user_history.txt\")\n",
    "# infer_embedding_file = os.path.join(data_path, \"infer_embedding.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "In this example, let's go through a real case on how to apply DKN on a raw news dataset from the very beginning. We will download a copy of open-source MIND dataset, in its original raw format. Then we will process the raw data files into DKN's input data format, which is stated previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_zip, valid_zip = download_mind(size=MIND_SIZE, dest_path=data_path)\n",
    "# train_path, valid_path = extract_mind(train_zip, valid_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_behavior(df):  \n",
    "    userid_history = {}\n",
    "    sessions = []\n",
    "    \n",
    "    for userid, imp_time, click, imps in zip(\n",
    "        df.user_id, \n",
    "        df.time,\n",
    "        df.clicked_news,\n",
    "        df.impressions\n",
    "    ):\n",
    "        clicks = click.split(\" \")\n",
    "        pos = []\n",
    "        neg = []\n",
    "        imps = imps.split(\" \")\n",
    "        for imp in imps:\n",
    "            if imp.split(\"-\")[1] == \"1\":\n",
    "                pos.append(imp.split(\"-\")[0])\n",
    "            else:\n",
    "                neg.append(imp.split(\"-\")[0])\n",
    "        userid_history[userid] = clicks\n",
    "        sessions.append([userid, clicks, pos, neg])\n",
    "    return sessions, userid_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read＿testing_behavior(df):  \n",
    "    userid_history = {}\n",
    "    sessions = []\n",
    "    \n",
    "    for userid, imp_time, click, imps in zip(\n",
    "        df.user_id, \n",
    "        df.time,\n",
    "        df.clicked_news,\n",
    "        df.impressions\n",
    "    ):\n",
    "        clicks = click.split(\" \")\n",
    "        pos = []\n",
    "        neg = []\n",
    "        imps = imps.split(\" \")\n",
    "        for imp in imps:\n",
    "            # if imp.split(\"-\")[1] == \"1\":\n",
    "            pos.append(imp)\n",
    "            # else:\n",
    "            #     neg.append(imp.split(\"-\")[0])\n",
    "        userid_history[userid] = clicks\n",
    "        sessions.append([userid, clicks, pos, neg])\n",
    "    return sessions, userid_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_news(df, news_words, news_entities, tokenizer):\n",
    "    for news_id, category, subcategory, title, abstract, URL, title_entities, abstract_entities in zip(\n",
    "        df.news_id,\n",
    "        df.category,\n",
    "        df.subcategory,\n",
    "        df.title,\n",
    "        df.abstract,\n",
    "        df.URL,\n",
    "        df.title_entities,\n",
    "        df.abstract_entities\n",
    "    ):\n",
    "        news_words[news_id] = tokenizer.tokenize(title.lower())\n",
    "        news_entities[news_id] = []\n",
    "        for entity in json.loads(title_entities):\n",
    "            news_entities[news_id].append(\n",
    "                (entity[\"SurfaceForms\"], entity[\"WikidataId\"])\n",
    "            )\n",
    "    return news_words, news_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_and_entities(df):\n",
    "    \"\"\"Load words and entities\n",
    "\n",
    "    Args:\n",
    "        train_news (str): News train file.\n",
    "        valid_news (str): News validation file.\n",
    "\n",
    "    Returns:\n",
    "        dict, dict: Words and entities dictionaries.\n",
    "    \"\"\"\n",
    "    news_words = {}\n",
    "    news_entities = {}\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    news_words, news_entities = _read_news(\n",
    "        df, news_words, news_entities, tokenizer\n",
    "    )\n",
    "    # news_words, news_entities = _read_news(\n",
    "    #     valid_new_df, news_words, news_entities, tokenizer\n",
    "    # )\n",
    "    return news_words, news_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _newsample(nnn, ratio):\n",
    "    if ratio > len(nnn):\n",
    "        return random.sample(nnn * (ratio // len(nnn) + 1), ratio)\n",
    "    else:\n",
    "        return random.sample(nnn, ratio)\n",
    "        \n",
    "def get_train_input(session, train_file_path, npratio=4):\n",
    "    \"\"\"Generate train file.\n",
    "\n",
    "    Args:\n",
    "        session (list): List of user session with user_id, clicks, positive and negative interactions.\n",
    "        train_file_path (str): Path to file.\n",
    "        npration (int): Ratio for negative sampling.\n",
    "    \"\"\"\n",
    "    fp_train = open(train_file_path, \"w\", encoding=\"utf-8\")\n",
    "    for sess_id in range(len(session)):\n",
    "        sess = session[sess_id]\n",
    "        userid, _, poss, negs = sess\n",
    "        for i in range(len(poss)):\n",
    "            pos = poss[i]\n",
    "            neg = _newsample(negs, npratio)\n",
    "            fp_train.write(\"1 \" + \"train_\" + userid + \" \" + pos + \"\\n\")\n",
    "            for neg_ins in neg:\n",
    "                fp_train.write(\"0 \" + \"train_\" + userid + \" \" + neg_ins + \"\\n\")\n",
    "    fp_train.close()\n",
    "    if os.path.isfile(train_file_path):\n",
    "        print(f\"Train file {train_file_path} successfully generated\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Error when generating {train_file_path}\")\n",
    "\n",
    "def get_val_input(session, train_file_path, npratio=4):\n",
    "    \"\"\"Generate train file.\n",
    "\n",
    "    Args:\n",
    "        session (list): List of user session with user_id, clicks, positive and negative interactions.\n",
    "        train_file_path (str): Path to file.\n",
    "        npration (int): Ratio for negative sampling.\n",
    "    \"\"\"\n",
    "    fp_train = open(train_file_path, \"w\", encoding=\"utf-8\")\n",
    "    for sess_id in range(len(session)):\n",
    "        sess = session[sess_id]\n",
    "        userid, _, poss, negs = sess\n",
    "        for i in range(len(poss)):\n",
    "            pos = poss[i]\n",
    "            neg = _newsample(negs, npratio)\n",
    "            fp_train.write(\"1 \" + \"valid_\" + userid + \" \" + pos + \"\\n\")\n",
    "            for neg_ins in neg:\n",
    "                fp_train.write(\"0 \" + \"valid_\" + userid + \" \" + neg_ins + \"\\n\")\n",
    "    fp_train.close()\n",
    "    if os.path.isfile(train_file_path):\n",
    "        print(f\"Train file {train_file_path} successfully generated\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Error when generating {train_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_input(session, train_file_path, npratio=4):\n",
    "    \"\"\"Generate train file.\n",
    "\n",
    "    Args:\n",
    "        session (list): List of user session with user_id, clicks, positive and negative interactions.\n",
    "        train_file_path (str): Path to file.\n",
    "        npration (int): Ratio for negative sampling.\n",
    "    \"\"\"\n",
    "    fp_train = open(train_file_path, \"w\", encoding=\"utf-8\")\n",
    "    for sess_id in range(len(session)):\n",
    "        sess = session[sess_id]\n",
    "        userid, _, poss, negs = sess\n",
    "        for i in range(len(poss)):\n",
    "            pos = poss[i]\n",
    "            # neg = _newsample(negs, npratio)\n",
    "            fp_train.write(\"1 \" + \"test_\" + userid + \" \" + pos + \"\\n\")\n",
    "            # for neg_ins in neg:\n",
    "            #     fp_train.write(\"0 \" + \"test_\" + userid + \" \" + neg_ins + \"\\n\")\n",
    "    fp_train.close()\n",
    "    if os.path.isfile(train_file_path):\n",
    "        print(f\"Train file {train_file_path} successfully generated\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Error when generating {train_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_history(train_history, valid_history, test_history, user_history_path):\n",
    "    \"\"\"Generate user history file.\n",
    "\n",
    "    Args:\n",
    "        train_history (list): Train history.\n",
    "        valid_history (list): Validation history\n",
    "        user_history_path (str): Path to file.\n",
    "    \"\"\"\n",
    "    fp_user_history = open(user_history_path, \"w\", encoding=\"utf-8\")\n",
    "    for userid in train_history:\n",
    "        fp_user_history.write(\n",
    "            \"train_\" + userid + \" \" + \",\".join(train_history[userid]) + \"\\n\"\n",
    "        )\n",
    "    for userid in valid_history:\n",
    "        fp_user_history.write(\n",
    "            \"valid_\" + userid + \" \" + \",\".join(valid_history[userid]) + \"\\n\"\n",
    "        )\n",
    "    for userid in test_history:\n",
    "        fp_user_history.write(\n",
    "            \"test_\" + userid + \" \" + \",\".join(test_history[userid]) + \"\\n\"\n",
    "        )\n",
    "    fp_user_history.close()\n",
    "    if os.path.isfile(user_history_path):\n",
    "        print(f\"User history file {user_history_path} successfully generated\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Error when generating {user_history_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./train/train_behaviors.tsv\", sep='\\t')\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2)\n",
    "# train_df = train_df.iloc[:500]\n",
    "# valid_df = valid_df.iloc[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"./test/test_behaviors.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_session, train_history = read_behavior(train_df)\n",
    "val_session, val_history = read_behavior(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_session, test_history = read_testing_behavior(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train file ./train/train.txt successfully generated\n",
      "Train file ./train/val.txt successfully generated\n"
     ]
    }
   ],
   "source": [
    "get_train_input(train_session, \"./train/train.txt\")\n",
    "get_val_input(val_session, \"./train/val.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train file ./test/test.txt successfully generated\n"
     ]
    }
   ],
   "source": [
    "get_test_input(test_session, \"./test/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User history file ./train/user_history.txt successfully generated\n"
     ]
    }
   ],
   "source": [
    "get_user_history(train_history, val_history, test_history, \"./train/user_history.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_history_file = \"./train/user_history.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news = pd.read_csv(\"./train/train_news.tsv\", sep='\\t')\n",
    "# display(train_news.query(\"title_entities.isnull()\"))\n",
    "train_news[\"title_entities\"] = train_news[\"title_entities\"].fillna(\"[]\")\n",
    "# train_news = train_news.query(\"~title_entities.isnull()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_news = pd.read_csv(\"./test/test_news.tsv\", sep='\\t')\n",
    "test_news[\"title_entities\"] = test_news[\"title_entities\"].fillna(\"[]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_news = pd.read_csv(\"./train/train_news.tsv\", sep='\\t')\n",
    "\n",
    "train_news_df, valid_news_df = train_test_split(train_news, test_size=0.2)\n",
    "news_words, news_entities = get_words_and_entities(pd.concat([train_news_df, valid_news_df, test_news]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_entities = os.path.join(\"./train/train_entity_embedding.vec\")\n",
    "valid_entities = os.path.join(\"./train/train_entity_embedding.vec\")\n",
    "news_feature_file, word_embeddings_file, entity_embeddings_file = generate_embeddings(\n",
    "    \"./train\",\n",
    "    news_words,\n",
    "    news_entities,\n",
    "    train_entities,\n",
    "    valid_entities,\n",
    "    max_sentence=10,\n",
    "    word_embedding_dim=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file = maybe_download(url=\"https://recodatasets.z20.web.core.windows.net/deeprec/deeprec/dkn/dkn_MINDsmall.yaml\", \n",
    "                           work_directory=\"./train\")\n",
    "hparams = prepare_hparams(yaml_file,\n",
    "                          news_feature_file=news_feature_file,\n",
    "                          user_history_file=user_history_file,\n",
    "                          wordEmb_file=word_embeddings_file,\n",
    "                          entityEmb_file=entity_embeddings_file,\n",
    "                          epochs=epochs,\n",
    "                          history_size=history_size,\n",
    "                          batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chilin/.pyenv/versions/3.9.13/envs/Medical/lib/python3.9/site-packages/recommenders/models/deeprec/models/dkn.py:305: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  curr_attention_layer = tf.compat.v1.layers.batch_normalization(\n",
      "/home/chilin/.pyenv/versions/3.9.13/envs/Medical/lib/python3.9/site-packages/recommenders/models/deeprec/models/dkn.py:193: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  curr_hidden_nn_layer = tf.compat.v1.layers.batch_normalization(\n",
      "2024-06-17 00:21:01.181599: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-17 00:21:01.181764: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-17 00:21:01.181819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-17 00:21:01.181940: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-17 00:21:01.181994: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-06-17 00:21:01.182033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10230 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model = DKN(hparams, DKNTextIterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 10000 , total_loss: 0.4534, data_loss: 0.4530\n",
      "at epoch 1\n",
      "train info: logloss loss:0.4612756512581832\n",
      "eval info: auc:0.711, group_auc:0.711, mean_mrr:0.0001, ndcg@10:0.8416, ndcg@5:0.8539\n",
      "at epoch 1 , train time: 1027.5 eval time: 73.4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain/train.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain/val.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/Medical/lib/python3.9/site-packages/recommenders/models/deeprec/models/base_model.py:465\u001b[0m, in \u001b[0;36mBaseModel.fit\u001b[0;34m(self, train_file, valid_file, test_file)\u001b[0m\n\u001b[1;32m    459\u001b[0m train_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (\n\u001b[1;32m    461\u001b[0m     batch_data_input,\n\u001b[1;32m    462\u001b[0m     impression,\n\u001b[1;32m    463\u001b[0m     data_size,\n\u001b[1;32m    464\u001b[0m ) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator\u001b[38;5;241m.\u001b[39mload_data_from_file(train_file):\n\u001b[0;32m--> 465\u001b[0m     step_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_sess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_data_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m     (_, _, step_loss, step_data_loss, summary) \u001b[38;5;241m=\u001b[39m step_result\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39mwrite_tfevents:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/Medical/lib/python3.9/site-packages/recommenders/models/deeprec/models/base_model.py:379\u001b[0m, in \u001b[0;36mBaseModel.train\u001b[0;34m(self, sess, feed_dict)\u001b[0m\n\u001b[1;32m    377\u001b[0m feed_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_keeps] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_prob_train\n\u001b[1;32m    378\u001b[0m feed_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_train_stage] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextra_update_ops\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerged\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/Medical/lib/python3.9/site-packages/tensorflow/python/client/session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    970\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[1;32m    971\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/Medical/lib/python3.9/site-packages/tensorflow/python/client/session.py:1191\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[0;32m-> 1191\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1194\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/Medical/lib/python3.9/site-packages/tensorflow/python/client/session.py:1371\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1368\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1371\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/Medical/lib/python3.9/site-packages/tensorflow/python/client/session.py:1378\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m   1377\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1379\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1380\u001b[0m     message \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_text(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/Medical/lib/python3.9/site-packages/tensorflow/python/client/session.py:1361\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[1;32m   1359\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[0;32m-> 1361\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_tf_sessionrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.13/envs/Medical/lib/python3.9/site-packages/tensorflow/python/client/session.py:1454\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1453\u001b[0m                         run_metadata):\n\u001b[0;32m-> 1454\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1456\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(\"train/train.txt\", \"train/val.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<recommenders.models.deeprec.models.dkn.DKN at 0x7f19e41cd550>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict('test/test.txt', 'result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "with open('result','r') as f:\n",
    "    for line in f:\n",
    "        result_list.append(line.split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "694980"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46332.0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "694980/15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>p5</th>\n",
       "      <th>p6</th>\n",
       "      <th>p7</th>\n",
       "      <th>p8</th>\n",
       "      <th>p9</th>\n",
       "      <th>p10</th>\n",
       "      <th>p11</th>\n",
       "      <th>p12</th>\n",
       "      <th>p13</th>\n",
       "      <th>p14</th>\n",
       "      <th>p15</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.36175606</td>\n",
       "      <td>0.04509889</td>\n",
       "      <td>0.17911519</td>\n",
       "      <td>0.53075224</td>\n",
       "      <td>0.06669196</td>\n",
       "      <td>0.083261505</td>\n",
       "      <td>0.29081</td>\n",
       "      <td>0.057372935</td>\n",
       "      <td>0.0811602</td>\n",
       "      <td>0.047518328</td>\n",
       "      <td>0.11509859</td>\n",
       "      <td>0.15095384</td>\n",
       "      <td>0.12012795</td>\n",
       "      <td>0.11362242</td>\n",
       "      <td>0.5369745</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.18135929</td>\n",
       "      <td>0.10899555</td>\n",
       "      <td>0.059672</td>\n",
       "      <td>0.16693313</td>\n",
       "      <td>0.26880142</td>\n",
       "      <td>0.094061226</td>\n",
       "      <td>0.18493365</td>\n",
       "      <td>0.21089295</td>\n",
       "      <td>0.45597854</td>\n",
       "      <td>0.22970092</td>\n",
       "      <td>0.21434873</td>\n",
       "      <td>0.0970766</td>\n",
       "      <td>0.35896006</td>\n",
       "      <td>0.1618471</td>\n",
       "      <td>0.19513726</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.119015545</td>\n",
       "      <td>0.3693068</td>\n",
       "      <td>0.4820874</td>\n",
       "      <td>0.059106633</td>\n",
       "      <td>0.28944468</td>\n",
       "      <td>0.12457587</td>\n",
       "      <td>0.4640087</td>\n",
       "      <td>0.14587691</td>\n",
       "      <td>0.2636757</td>\n",
       "      <td>0.18052788</td>\n",
       "      <td>0.29806885</td>\n",
       "      <td>0.54376644</td>\n",
       "      <td>0.3656495</td>\n",
       "      <td>0.09000298</td>\n",
       "      <td>0.15022962</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.46198213</td>\n",
       "      <td>0.031377327</td>\n",
       "      <td>0.17136249</td>\n",
       "      <td>0.26808223</td>\n",
       "      <td>0.26628342</td>\n",
       "      <td>0.5194608</td>\n",
       "      <td>0.23092362</td>\n",
       "      <td>0.5174912</td>\n",
       "      <td>0.14719352</td>\n",
       "      <td>0.2061417</td>\n",
       "      <td>0.06942703</td>\n",
       "      <td>0.23925517</td>\n",
       "      <td>0.30125222</td>\n",
       "      <td>0.2624396</td>\n",
       "      <td>0.33627108</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.031812295</td>\n",
       "      <td>0.4957133</td>\n",
       "      <td>0.12268076</td>\n",
       "      <td>0.18156593</td>\n",
       "      <td>0.09164113</td>\n",
       "      <td>0.19375113</td>\n",
       "      <td>0.15019779</td>\n",
       "      <td>0.26275617</td>\n",
       "      <td>0.12086844</td>\n",
       "      <td>0.122133136</td>\n",
       "      <td>0.053790092</td>\n",
       "      <td>0.08981569</td>\n",
       "      <td>0.37976313</td>\n",
       "      <td>0.10551541</td>\n",
       "      <td>0.23934072</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46327</th>\n",
       "      <td>0.06995096</td>\n",
       "      <td>0.049044095</td>\n",
       "      <td>0.30138427</td>\n",
       "      <td>0.19764103</td>\n",
       "      <td>0.19944179</td>\n",
       "      <td>0.23753273</td>\n",
       "      <td>0.0858201</td>\n",
       "      <td>0.24321157</td>\n",
       "      <td>0.44900772</td>\n",
       "      <td>0.23109291</td>\n",
       "      <td>0.09677192</td>\n",
       "      <td>0.57003915</td>\n",
       "      <td>0.3928931</td>\n",
       "      <td>0.12638804</td>\n",
       "      <td>0.15141757</td>\n",
       "      <td>46327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46328</th>\n",
       "      <td>0.03916842</td>\n",
       "      <td>0.11566103</td>\n",
       "      <td>0.2755931</td>\n",
       "      <td>0.14698729</td>\n",
       "      <td>0.43751428</td>\n",
       "      <td>0.098587655</td>\n",
       "      <td>0.21313521</td>\n",
       "      <td>0.13319813</td>\n",
       "      <td>0.24292974</td>\n",
       "      <td>0.32554367</td>\n",
       "      <td>0.12606357</td>\n",
       "      <td>0.43438226</td>\n",
       "      <td>0.073093645</td>\n",
       "      <td>0.026386108</td>\n",
       "      <td>0.12904869</td>\n",
       "      <td>46328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46329</th>\n",
       "      <td>0.121474564</td>\n",
       "      <td>0.24804963</td>\n",
       "      <td>0.6095199</td>\n",
       "      <td>0.16438577</td>\n",
       "      <td>0.07415296</td>\n",
       "      <td>0.33927238</td>\n",
       "      <td>0.055135734</td>\n",
       "      <td>0.0981009</td>\n",
       "      <td>0.5243851</td>\n",
       "      <td>0.12745003</td>\n",
       "      <td>0.4761079</td>\n",
       "      <td>0.09974697</td>\n",
       "      <td>0.063629456</td>\n",
       "      <td>0.3440112</td>\n",
       "      <td>0.3333002</td>\n",
       "      <td>46329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46330</th>\n",
       "      <td>0.5147637</td>\n",
       "      <td>0.1249591</td>\n",
       "      <td>0.1431812</td>\n",
       "      <td>0.4633263</td>\n",
       "      <td>0.2303275</td>\n",
       "      <td>0.2269931</td>\n",
       "      <td>0.24251199</td>\n",
       "      <td>0.25169572</td>\n",
       "      <td>0.29808298</td>\n",
       "      <td>0.1235264</td>\n",
       "      <td>0.0899706</td>\n",
       "      <td>0.12469008</td>\n",
       "      <td>0.0955164</td>\n",
       "      <td>0.25364244</td>\n",
       "      <td>0.10863317</td>\n",
       "      <td>46330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46331</th>\n",
       "      <td>0.072936475</td>\n",
       "      <td>0.17874163</td>\n",
       "      <td>0.43988857</td>\n",
       "      <td>0.08562897</td>\n",
       "      <td>0.23072721</td>\n",
       "      <td>0.18800572</td>\n",
       "      <td>0.358548</td>\n",
       "      <td>0.27125722</td>\n",
       "      <td>0.2545959</td>\n",
       "      <td>0.103591196</td>\n",
       "      <td>0.30413306</td>\n",
       "      <td>0.22059044</td>\n",
       "      <td>0.15525493</td>\n",
       "      <td>0.29720962</td>\n",
       "      <td>0.24765536</td>\n",
       "      <td>46331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46332 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                p1           p2          p3           p4          p5  \\\n",
       "0       0.36175606   0.04509889  0.17911519   0.53075224  0.06669196   \n",
       "1       0.18135929   0.10899555    0.059672   0.16693313  0.26880142   \n",
       "2      0.119015545    0.3693068   0.4820874  0.059106633  0.28944468   \n",
       "3       0.46198213  0.031377327  0.17136249   0.26808223  0.26628342   \n",
       "4      0.031812295    0.4957133  0.12268076   0.18156593  0.09164113   \n",
       "...            ...          ...         ...          ...         ...   \n",
       "46327   0.06995096  0.049044095  0.30138427   0.19764103  0.19944179   \n",
       "46328   0.03916842   0.11566103   0.2755931   0.14698729  0.43751428   \n",
       "46329  0.121474564   0.24804963   0.6095199   0.16438577  0.07415296   \n",
       "46330    0.5147637    0.1249591   0.1431812    0.4633263   0.2303275   \n",
       "46331  0.072936475   0.17874163  0.43988857   0.08562897  0.23072721   \n",
       "\n",
       "                p6           p7           p8          p9          p10  \\\n",
       "0      0.083261505      0.29081  0.057372935   0.0811602  0.047518328   \n",
       "1      0.094061226   0.18493365   0.21089295  0.45597854   0.22970092   \n",
       "2       0.12457587    0.4640087   0.14587691   0.2636757   0.18052788   \n",
       "3        0.5194608   0.23092362    0.5174912  0.14719352    0.2061417   \n",
       "4       0.19375113   0.15019779   0.26275617  0.12086844  0.122133136   \n",
       "...            ...          ...          ...         ...          ...   \n",
       "46327   0.23753273    0.0858201   0.24321157  0.44900772   0.23109291   \n",
       "46328  0.098587655   0.21313521   0.13319813  0.24292974   0.32554367   \n",
       "46329   0.33927238  0.055135734    0.0981009   0.5243851   0.12745003   \n",
       "46330    0.2269931   0.24251199   0.25169572  0.29808298    0.1235264   \n",
       "46331   0.18800572     0.358548   0.27125722   0.2545959  0.103591196   \n",
       "\n",
       "               p11         p12          p13          p14         p15     id  \n",
       "0       0.11509859  0.15095384   0.12012795   0.11362242   0.5369745      0  \n",
       "1       0.21434873   0.0970766   0.35896006    0.1618471  0.19513726      1  \n",
       "2       0.29806885  0.54376644    0.3656495   0.09000298  0.15022962      2  \n",
       "3       0.06942703  0.23925517   0.30125222    0.2624396  0.33627108      3  \n",
       "4      0.053790092  0.08981569   0.37976313   0.10551541  0.23934072      4  \n",
       "...            ...         ...          ...          ...         ...    ...  \n",
       "46327   0.09677192  0.57003915    0.3928931   0.12638804  0.15141757  46327  \n",
       "46328   0.12606357  0.43438226  0.073093645  0.026386108  0.12904869  46328  \n",
       "46329    0.4761079  0.09974697  0.063629456    0.3440112   0.3333002  46329  \n",
       "46330    0.0899706  0.12469008    0.0955164   0.25364244  0.10863317  46330  \n",
       "46331   0.30413306  0.22059044   0.15525493   0.29720962  0.24765536  46331  \n",
       "\n",
       "[46332 rows x 16 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "result_df = pd.DataFrame(np.array(result_list).reshape(len(result_list) // 15, 15), columns=[f'p{i+1}' for i in range(15)])\n",
    "result_df['id'] = range(len(result_df))\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "3a9a0c422ff9f08d62211b9648017c63b0a26d2c935edc37ebb8453675d13bb5"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
